
\section{Data and Monte Carlo Simulation}
\subsection{Data Samples}
Brief overview of Full Run 2 and where the data comes from and what time period.\\\\
Luminosity of full run 2. 

\subsection{Monte Carlo Samples}
Signal and Background samples. Choice of cuts at ntuple level. Specifically why each background is chosen (and why others excluded) $\rightarrow$ e.g. talk about branching fractions, cross sections, topology. How are these backgrounds passing our event selection (e.g. ttz $\rightarrow$ b can be lost/untagged/mis-id'ed) $\rightarrow$ provide an explanation for each background.\\\\
Details of each sample (event generator, parton shower).


The following background processes are considered:
\begin{itemize}
	\item $\mathbf{t \bar{t} Z}$: $t \bar{t} $  with an associated $Z$-boson, in the tetralepton final state. Therefore, both top-quarks decay leptonically (e.g. $t \rightarrow W^+ b \rightarrow \ell^+ \nu b$) and of these top-quarks emits a $Z$-boson which decays leptonically ($Z \rightarrow \ell^\pm \ell^\mp$ (OSSF lepton pair) ). This results in a final state with 4 leptons and 2 b-quarks.
	\item $\mathbf{ZZ}$: Diboson production with a tetralepton final state, therefore both $Z$-bosons decay leptonically ($Z \rightarrow \ell^\pm \ell^\mp$ (OSSF lepton pair) ).
	\item \textbf{other}: Processes with a relatively minimal, but non-negligible background contribution in the \tWZ signal region 
	\begin{itemize}
	\item [-] Triboson: 
	\begin{itemize}
		\item [] $WZZ \rightarrow \ell\ell\ell\ell\ell \nu$
		\item [] $ZZZ \rightarrow \ell \ell \ell\ell\ell\ell$
		\item [] $ZZZ \rightarrow \ell\ell\ell\ell\nu\nu$
		\item [] $WWZ \rightarrow \ell\ell\ell\ell\nu\nu$
	\end{itemize}
	\item [-] tZq: Top quark in association with a $Z$-boson and another quark.
	 
	\end{itemize}
\end{itemize}




\section{Objects}
In the subsections below:\\\\
Explain why we applied each cut/selection.\\\\


\subsection{Leptons}
Tight/loose/med definitions, efficiency of electron and muons specifically at ATLAS. Why we don't consider taus.


In addition to our selection criteria of exactly four tight leptons, we require that the leading (L), next-to-leading (NL), next-to-next-to-leading (NNL) and next-to-next-to-next-to-leading (NNNL) leptons have $p_{T}$ greater than $\SI{28}{}$, $\SI{10}{}$, $\SI{10}{}$ and $\SI{10}{\GeV}$ respectively. Here we have chosen to apply looser object-level cuts than the tri-lepton channel in an attempt to maximize our signal statistics, as the tetra-lepton channel is heavily statistically limited.\\\\
Reconstructed electrons are required to be within $|\eta| < 2.47$ and excluding the transition region between the barrel and end-cap calorimeters at $1.37 < |\eta| < 1.52$. Reconstructed muons are required to be within $|\eta| < 2.5$.
\subsection{Jets}
What algorithm did we use and why, loose and tight jet definitions

Jets are required to be within $|\eta| < 2.5$ and $p_{T}(\text{jet}) > \SI{25}{\GeV}$. We apply these looser $p_T$ cuts in an attempt to increase our limited signal statistics. The jet-vertex-tagger (jvt) on jets are required to have a value greater than $\SI{0.5}{}$, in an attempt to reject effects caused by pile-up interactions. (** more detail to go here about what jvt is, why do we require this, more info on how electrons and muons are reconstructed and why we apply these selections **)

\subsection{b-tagging}
What algorithm/WP did we use and why
\section{Kinematic Pre-selection cuts}
Mass windows on Z (OSSF), sum charge $=$ 0, explanations on all other non object level cuts/selections, OSSF $<$ 10 GeV cut


The invariant mass of the OSSF lepton pair coming from the $Z$ boson must equal the invariant mass of the $Z$ boson, and noting that lepton reconstruction and identification in the ATLAS detector has a high accuracy ~\cite{}, we can use these OSSF leptons to reconstruct the $Z$ boson with relatively high confidence. We therefore define a $Z$ candidate as an OSSF lepton pair with an invariant mass, $m_{\text{OSSF}}$, satisfying the condition, $|m_{\text{OSSF}} - m_Z| <  \SI{30}{\GeV}$, where $m_Z$ is the nominal $Z$ boson mass ($\SI{91.1876}{\GeV}$ ~\cite{pdg}). Multiple $Z$ candidates can be present in certain decay channels (e.g. $eeee$, $\mu\mu ee$, $\mu \mu \mu \mu$). In these cases, the $Z$ candidate which has an invariant mass closest to the nominal $Z$ boson mass is chosen.\\\\
In order to suppress quarkonia (low mass resonances such as $J/\psi$ and upsilon) we require that all OSSF lepton pairs have an invariant mass, $m_{\text{OSSF}}$, greater than $\SI{10}{\GeV}$.\\\\
Due to conservation of charge, the final state lepton charges must sum to zero.\\ We therefore require $\displaystyle\sum_{i=1}^{4} \text{charge}(\ell_i) = 0$.



\section{Regions and Event Selection}
\label{sec:regionsAndEventSelection}


The selection criteria which define the SR and the CRs are summarised in Table ~\ref{tab:4Lep-cutsummary}. In order to check the modelling of the most dominant background components in our signal region, we have modified our selection criteria to define \ttZ and $ZZb$ control regions. The \ttZ control region has the same requirement on the number of reconstructed $Z$ boson candidates in the signal region (due to a commonality on the number of $Z$ bosons present in both processes), however we require at least two jets and that exactly two of these jets are b-tagged (corresponding to the $b$-quark jets originating from the two top-quark decays). We choose to define a $ZZb$ region, as opposed to a \ZZ region, since the \ZZ background present in the $tWZ$ signal region contains exactly one b-tagged jet. Therefore defining a region with \ZZ plus exactly one $b$-quark more closely resembles the $ZZ$ background present in the signal region. In addition to this, mis-modelling of \ZZ has been seen in other analyses ~\cite{Aaboud:2019, ppToZZ:CMSpaper}, further motivating the use of a $ZZb$ control region over a \ZZ CR. The $ZZb$ CR requires exactly two $Z$ boson candidates and exactly one b-tagged jet, with no requirement on the number of jets.




\begin{table}[htbp]
	
	
	\centering
	\begin{tabular}{m{3.5cm}m{3.5cm}m{3.5cm}}
		\toprule
		\multicolumn{3}{c}{Common selections}  \\
		\midrule
		\multicolumn{3}{c}{Exactly 4 tight leptons}   \\
		%  \multicolumn{3}{c}{$\pT(\ell_1)> \SI{28}{\GeV}$, $\pT(\ell_2)> \SI{10}{\GeV}$, $\pT(\ell_3)> \SI{10}{\GeV}$, $\pT(\ell_4)> \SI{10}{\GeV}$}   \\
		\multicolumn{3}{c}{$\pT(\ell_1,\ell_2,\ell_3,\ell_4) > (28,10,10,10)$ $\SI{}{\GeV}$}   \\
		\multicolumn{3}{c}{$\pT(\text{jet})> \SI{25}{\GeV}$, $|\eta(\text{jet})| < \SI{2.5}{}$, $\text{jvt} > \SI{0.5}{}$}   \\
		\multicolumn{3}{c}{$|\eta(\ell_e)| < \SI{2.47}{}$ excluding $\SI{1.37}{} < |\eta(\ell_e)| < \SI{1.52}{}$}   \\
		\multicolumn{3}{c}{$|\eta(\ell_\mu)| < \SI{2.5}{}$}   \\
		\multicolumn{3}{c}{$\displaystyle\sum_{i=1}^{4} \text{charge}(\ell_i) = 0$}   \\
		\multicolumn{3}{c}{All OSSF lepton pairs require $m_{\text{OSSF}} > \SI{10}{\GeV}$}   \\
		\bottomrule
		SR 1z1b & \ttZ 1z2b CR & $ZZb$ 2z1b CR\\
		\midrule
		1 $Z$ candidate & 1 $Z$ candidate & 2 $Z$ candidates\\
		$\geq$ 1 jet & $\geq$ 2 jets & no requirement\\
		exactly 1 b-tagged jet & exactly 2 b-tagged jets & exactly 1 b-tagged jet\\
		\bottomrule
	\end{tabular}
	\caption{
		Overview of the requirements applied for selecting events in the signal and control regions in the tetralepton channel
	}%
	\label{tab:4Lep-cutsummary}
\end{table}


Summary table of event selection. Why chose ZZb and ttz region. 
\section{Machine Learning Techniques}
What tool did we use, how did we use it, parameters of bdt/nn, input variables/importance, conversion of event level bdt output (bdtscore) to variable for fitting. Used ROC curve integral as a proxy for how good bdt was doing. 
\section{Fake Lepton Estimation} 
Expected to be a small effect, why? Brief, general explanation/idea of methods used (full explanation/description of what we did and the results/plots/etc. later) 
\section{Analysis Pipeline/Workflow and TRExFitter}
What is TRExFitter? What can it do? At which stage(s) in the analysis did we use it? Which version did we use? Binning method. Explain calculation of error bars in TRF. \\\\ 
Include general flow chart of analysis (not sure where)

We make use of industry standard \texttt{ROOT}\footnote{CERN's HEP data analysis framework (written in \texttt{C$\+\+$})} wrappers in this analysis, namely, \texttt{PyROOT} and \texttt{TRExFitter}.\\\\
\texttt{Python} is used extensively in many fields of science (not limited to physics and data science) due to its simplicity and ongoing support by the communities which utilize it. \texttt{PyROOT} allows users to access the full \texttt{ROOT} functionality within \texttt{Python}. More specifically, \texttt{PyROOT} provides \texttt{Python} bindings for \texttt{ROOT}.\\\\
\texttt{TRExFitter} is a framework for binned template profile likelihood fits\cite{TRexfitter}. In this analysis, we used TRExFitter (tag: \texttt{TRExFitter-00-04-11}) to produce all pre-fit and post-fit plots (including fit statistics, e.g. limit, significance, $\mu_{best-fit}$).\\\\
The analysis pipeline starts with sample derivations (derived dataset) being submitted to the grid for ntuple production. This applies cuts and selections to the already reduced derivations and produces ntuples with trees containing variables (e.g. scale factors, observables, MC truth flags) that will be used at future stages in the analysis. These ntuples are then read by \texttt{PyROOT} where the events are looped over, before being written to \texttt{ROOT} files as input to \texttt{TRExFitter}. The \texttt{Python} script's main purpose is to define the different regions and apply the final cuts and selections outlined in Table \ref{tab:4Lep-cutsummary}. As each event is looped over, these cut and selection criteria are checked for the given event and is either thrown away, or gets written to a \texttt{ROOT} file corresponding to the MC sample and Full Run 2 data-set (mc16a, mc16d, mc16e) which it belongs to. \texttt{TRExFitter} then takes these files as input, runs a maximum likelihood fit and produces relevant plots (e.g. pre-fit, post-fit, pull plots) and statistical parameters (e.g. limit, significance, $\mu_{best-fit}$).\\\\
Throughout this analysis, we ensured that the signal region is blinded. We did this by implementing \texttt{TRExFitter's} \textit{'mixed data and MC'}~\cite{MixedDataAndMC_TRF} fit, which aims to obtain the most accurate prediction for the expected results (while keeping the signal region blinded). It does this by performing a background only fit to the control regions (using real data). The set of fitted values for all the nuisance parameters from the background only fit are then used to construct a modified ASIMOV data-set. Finally, the fit is performed using real data in the control regions and the aforementioned modified ASIMOV data-set in the signal region.








